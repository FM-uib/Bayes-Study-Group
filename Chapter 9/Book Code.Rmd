---
title: "Chapter 9 - Book Code"
output: html_notebook
---

# Chapter 9: Normal one-way ANOVA 
Load required Packages for this Chapter and set a seed for reproducability

```{r, message=FALSE, warning=FALSE}
library(here)
library(rjags)
library("lme4")
set.seed(101)
```

# 9.2 Fixed Effect ANOVA
First we generate the necessary data.

```{r}
ngroups <- 5				                  # Number of populations
nsample <- 10			                	  # Number of snakes in each
pop.means <- c(50, 40, 45, 55, 60) 	  # Population mean SVL
sigma <- 3				                    # Residual sd

n <- ngroups * nsample 			          # Total number of data points
eps <- rnorm(n, 0, sigma)	           	# Residuals 
x <- rep(1:5, rep(nsample, ngroups)) 	# Indicator for population
means <- rep(pop.means, rep(nsample, ngroups))
X <- as.matrix(model.matrix(~ as.factor(x)-1)) # Create design matrix
y <- as.numeric(X %*% as.matrix(pop.means) + eps) # assemble -- NOTE: as.numeric ESSENTIAL for WinBUGS

boxplot(y~x, col="grey", xlab="Population", ylab="SVL", main="", las = 1)

```

# 9.2.2. Maximum likelihood analysis using R
We recognize the input variables fairly well in the results.
R does effect parameterization as default. It is reparameterized to means in the code below to increase clarity.
```{r}
lm<-lm(y~as.factor(x)-1)
print(anova(lm(y~as.factor(x)-1)))
cat("\n\n")
print(lm$coeff, dig = 3)
cat("Sigma:         ",summary(lm)$sigma, "\n\n")
rbind(lm$coefficients,pop.means)
```

# 9.2.3. Bayesian analysis
Model description.
The SVL (y) is normaly distributed around a mean. This mean depends on a population mean (pop.mean). These pop.means are independent of eachother. Our assumption for the pop.means is uniform distribution. 

We further define custom hypotheses to test the differences between populations.
Test 1 compares population 2/3 with 4/5. Test 2 tests if the effect of population 5 is 2 times that of population 4. 
```{r}
model<-"
model {

# Likelihood
 for (i in 1:50) {
    y[i] ~ dnorm(mean[i], tau) 
    mean[i] <- pop.mean[x[i]]
 }

# Priors
 for (i in 1:5){			# Implicitly define alpha as a vector
    pop.mean[i] ~ dunif(0, 100)
 }
 sigma ~ dunif(0, 100)

# Derived quantities
 tau <- 1 / ( sigma * sigma)
 effe2 <- pop.mean[2] - pop.mean[1]
 effe3 <- pop.mean[3] - pop.mean[1]
 effe4 <- pop.mean[4] - pop.mean[1]
 effe5 <- pop.mean[5] - pop.mean[1]

# Custom hypothesis test / Define your own contrasts
 test1 <- (effe2+effe3) - (effe4+effe5) # Equals zero when 2+3 = 4+5
 test2 <- effe5 - 2 * effe4 		# Equals zero when effe5 = 2*effe4
}
"
model.spec <- textConnection(model)
```

Bundle all the necessary data and start the chains.

```{r, message=FALSE, warning=FALSE}
data <- list("y"= y, "x" = x)

inits <- function(){ list(pop.mean = runif(5, 0, 100), sigma = runif(1, 0, 100) )}

params <- c("pop.mean", "sigma", "effe2", "effe3", "effe4", "effe5", "test1", "test2")

jagsModel <- jags.model(file= textConnection(model),
                        data=data,
                        init = inits,
                        n.chains = 3, 
                        n.adapt = 100)
Samples <- coda.samples(jagsModel, 
                        variable.names = params, 
                        n.iter = 1000)
```

With uninformed priors we can reconstruct the starting values we used for generating the dataset and the values are very close to the population means from the liner model.
One advantage of Bayesian methods is the ease with which derived quantities can be estimated.
Seen in the tests we conducted in this example.

```{r}
mcmc <- data.frame(do.call(rbind, Samples))
means <-rbind(colMeans(mcmc)[5:9], lm$coefficients, pop.means)
rownames(means)<-c("Bayes", "lm", "data")

summary(Samples)

means
```

# 9.3 Random-Effects ANOVA

Data generation and plot.

```{r}
rm(list = ls())
set.seed(101)
npop <- 10				# Number of populations: now choose 10 rather than 5
nsample <- 12				# Number of snakes in each
n <- npop * nsample			# Total number of data points

pop.grand.mean <- 50			# Grand mean SVL
pop.sd <- 5				# sd of population effects about mean
pop.means <- rnorm(n = npop, mean = pop.grand.mean, sd = pop.sd)
sigma <- 3				# Residual sd
eps <- rnorm(n, 0, sigma) 		# Draw residuals

x <- rep(1:npop, rep(nsample, npop))
X <- as.matrix(model.matrix(~ as.factor(x)-1))
y <- as.numeric(X %*% as.matrix(pop.means) + eps) # as.numeric is ESSENTIAL
boxplot(y ~ x, col = "grey", xlab = "Population", ylab = "SVL", main = "", las = 1) # Plot of generated data
abline(h = pop.grand.mean)
```

# 9.3.2. Restricted maximum likelihood (REML) analysis using R

We are using a random effects ANOVA with the lme4 package.

```{r}
pop <- as.factor(x)			# Define x as a factor and call it pop

lme.fit <- lmer(y ~ 1 + 1 | pop, REML = TRUE)
lme.fit					# Inspect results
rbind(t(50+ranef(lme.fit)$pop, pop.means))
```

# 9.3.3 Bayesian random effects ANOVA

Next we fit a random effects model. 

```{r}
model<-"
model {
# Likelihood
 for (i in 1:n) {
    y[i] ~ dnorm(mean[i], tau.res)
    mean[i] <- pop.mean[x[i]]
 }

# Priors and some derived things
for (i in 1:npop){
    pop.mean[i] ~ dnorm(mu, tau.group) 	# Prior for population means
    effe[i] <- pop.mean[i] - mu 	# Population effects as derived quantâ€™s
 }
 mu ~ dunif(0,100)			# Hyperprior for grand mean svl
 sigma.group ~ dunif(0, 10)		# Hyperprior for sd of population effects
 sigma.res ~ dunif(0, 10)		# Prior for residual sd

# Derived quantities
 tau.group <- 1 / (sigma.group * sigma.group)
 tau.res <- 1 / (sigma.res * sigma.res)
}
"
model.spec <- textConnection(model)
```



```{r, message=FALSE, warning=FALSE}
data <- list("y"=y, "x"=x, "npop"= npop, "n" = n)

# Inits function
inits <- function(){ list(mu = runif(1, 0, 100), sigma.group = runif(1, 0, 10), sigma.res = runif(1, 0, 10) )}

# Params to estimate
params <- c("mu", "pop.mean", "effe", "sigma.group", "sigma.res")

jagsModel <- jags.model(file= textConnection(model),
                        data=data,
                        init = inits,
                        n.chains = 3, 
                        n.adapt = 100)
Samples <- coda.samples(jagsModel, 
                        variable.names = params, 
                        n.iter = 1000)
```

The estimates we get from the two models are quite similar.
The among population standard deviation (Bayes: sigma.group = 4.7; lme: pop Std.Dev = 4.1) is more accurately estimated using the Bayesian model, remember the truth was 5. This may be due to the small sample size. 

```{r}
mcmc <- do.call(rbind, Samples)
means <- rbind(colMeans(mcmc)[12:21], t(50+ranef(lme.fit)$pop), pop.means)
rownames(means) <- c("Bayes", "lme", "data")
lme.fit
summary(Samples)
means
```
